{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🚀 CekAjaYuk - Complete Model Training\n",
    "## Deteksi Lowongan Kerja Palsu dengan Machine Learning & Deep Learning\n",
    "\n",
    "**Notebook ini menggabungkan semua proses training:**\n",
    "1. 📂 Dataset Preparation & Validation\n",
    "2. 🔧 Data Preprocessing & Feature Extraction\n",
    "3. 🌲 Random Forest Training & Optimization\n",
    "4. 🧠 CNN/TensorFlow Deep Learning Training\n",
    "5. 📊 Model Evaluation & Export\n",
    "\n",
    "---\n",
    "### 📋 Requirements:\n",
    "- Dataset: 800 gambar (400 fake + 400 genuine)\n",
    "- Python 3.7+\n",
    "- Libraries: scikit-learn, tensorflow, opencv, pillow\n",
    "\n",
    "### 🎯 Output:\n",
    "- `random_forest_production.pkl`\n",
    "- `cnn_production.h5`\n",
    "- `feature_scaler_production.pkl`\n",
    "- `text_vectorizer_production.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 🔧 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "!pip install opencv-python-headless\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "!pip install pillow\n",
    "!pip install matplotlib seaborn\n",
    "!pip install joblib\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import joblib\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Flatten, Dropout, \n",
    "    GlobalAveragePooling2D, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"📚 All libraries imported successfully!\")\n",
    "print(f\"🔥 TensorFlow version: {tf.__version__}\")\n",
    "print(f\"🐍 Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_upload"
   },
   "source": [
    "## 📂 2. Dataset Upload & Preparation\n",
    "\n",
    "### 📋 Instructions:\n",
    "1. **Zip your dataset** dengan struktur:\n",
    "   ```\n",
    "   dataset.zip\n",
    "   ├── fake/\n",
    "   │   ├── fake_job_001.jpg\n",
    "   │   ├── fake_job_002.jpg\n",
    "   │   └── ... (400 files)\n",
    "   └── genuine/\n",
    "       ├── genuine_job_001.jpg\n",
    "       ├── genuine_job_002.jpg\n",
    "       └── ... (400 files)\n",
    "   ```\n",
    "2. **Upload** file `dataset.zip` ke Colab\n",
    "3. **Run** cell di bawah untuk extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_dataset"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"📤 Upload your dataset.zip file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract dataset\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"📦 Extracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(f\"✅ {filename} extracted successfully!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"❌ No zip file found. Please upload dataset.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_dataset"
   },
   "outputs": [],
   "source": [
    "# Validate dataset structure\n",
    "def validate_dataset(dataset_dir='dataset'):\n",
    "    \"\"\"Validate dataset structure and count files\"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"❌ Dataset directory '{dataset_dir}' not found!\")\n",
    "        return False\n",
    "    \n",
    "    fake_dir = dataset_path / 'fake'\n",
    "    genuine_dir = dataset_path / 'genuine'\n",
    "    \n",
    "    if not fake_dir.exists():\n",
    "        print(f\"❌ Fake directory not found: {fake_dir}\")\n",
    "        return False\n",
    "        \n",
    "    if not genuine_dir.exists():\n",
    "        print(f\"❌ Genuine directory not found: {genuine_dir}\")\n",
    "        return False\n",
    "    \n",
    "    # Count files\n",
    "    fake_files = list(fake_dir.glob('*.[jJ][pP][gG]')) + list(fake_dir.glob('*.[pP][nN][gG]'))\n",
    "    genuine_files = list(genuine_dir.glob('*.[jJ][pP][gG]')) + list(genuine_dir.glob('*.[pP][nN][gG]'))\n",
    "    \n",
    "    print(f\"📊 Dataset Validation:\")\n",
    "    print(f\"   📁 Fake samples: {len(fake_files)}\")\n",
    "    print(f\"   📁 Genuine samples: {len(genuine_files)}\")\n",
    "    print(f\"   📁 Total samples: {len(fake_files) + len(genuine_files)}\")\n",
    "    \n",
    "    if len(fake_files) == 0 or len(genuine_files) == 0:\n",
    "        print(\"❌ Dataset validation failed: Empty directories\")\n",
    "        return False\n",
    "    \n",
    "    print(\"✅ Dataset validation passed!\")\n",
    "    return True\n",
    "\n",
    "# Validate the uploaded dataset\n",
    "dataset_valid = validate_dataset()\n",
    "\n",
    "if not dataset_valid:\n",
    "    print(\"\\n🔄 Trying alternative dataset paths...\")\n",
    "    # Try different possible paths\n",
    "    for possible_path in ['data', 'datasets', '.']:\n",
    "        if validate_dataset(possible_path):\n",
    "            dataset_valid = True\n",
    "            break\n",
    "\n",
    "if not dataset_valid:\n",
    "    print(\"\\n❌ Dataset validation failed. Please check your upload.\")\n",
    "else:\n",
    "    print(\"\\n🎉 Ready to proceed with training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_extraction"
   },
   "source": [
    "## 🔧 3. Feature Extraction & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_extractor_class"
   },
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor:\n",
    "    \"\"\"Extract features from job posting images\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=(224, 224)):\n",
    "        self.img_size = img_size\n",
    "        self.supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "        \n",
    "    def load_and_preprocess_image(self, image_path):\n",
    "        \"\"\"Load and preprocess single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(str(image_path))\n",
    "            if img is None:\n",
    "                return None\n",
    "                \n",
    "            # Convert BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            \n",
    "            # Normalize\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            \n",
    "            return img\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_visual_features(self, img):\n",
    "        \"\"\"Extract visual features from image\"\"\"\n",
    "        if img is None:\n",
    "            return np.zeros(10)  # Return zero features if image is None\n",
    "            \n",
    "        features = []\n",
    "        \n",
    "        # Convert to different color spaces\n",
    "        gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        hsv = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # 1. Color statistics\n",
    "        features.extend([\n",
    "            np.mean(img[:,:,0]),  # Red mean\n",
    "            np.mean(img[:,:,1]),  # Green mean\n",
    "            np.mean(img[:,:,2]),  # Blue mean\n",
    "            np.std(img[:,:,0]),   # Red std\n",
    "            np.std(img[:,:,1]),   # Green std\n",
    "            np.std(img[:,:,2])    # Blue std\n",
    "        ])\n",
    "        \n",
    "        # 2. Brightness and contrast\n",
    "        features.extend([\n",
    "            np.mean(gray),        # Brightness\n",
    "            np.std(gray),         # Contrast\n",
    "        ])\n",
    "        \n",
    "        # 3. Edge density (Canny edges)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])\n",
    "        features.append(edge_density)\n",
    "        \n",
    "        # 4. Texture (using Laplacian variance)\n",
    "        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        features.append(laplacian_var)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def load_dataset(self, dataset_dir='dataset'):\n",
    "        \"\"\"Load complete dataset with features and labels\"\"\"\n",
    "        dataset_path = Path(dataset_dir)\n",
    "        \n",
    "        images = []\n",
    "        features = []\n",
    "        labels = []\n",
    "        filenames = []\n",
    "        \n",
    "        # Load fake samples\n",
    "        fake_dir = dataset_path / 'fake'\n",
    "        fake_files = list(fake_dir.glob('*.[jJ][pP][gG]')) + list(fake_dir.glob('*.[pP][nN][gG]'))\n",
    "        \n",
    "        print(f\"📂 Loading {len(fake_files)} fake samples...\")\n",
    "        for i, img_path in enumerate(fake_files):\n",
    "            img = self.load_and_preprocess_image(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                features.append(self.extract_visual_features(img))\n",
    "                labels.append(0)  # 0 = fake\n",
    "                filenames.append(img_path.name)\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"   Processed {i + 1}/{len(fake_files)} fake samples\")\n",
    "        \n",
    "        # Load genuine samples\n",
    "        genuine_dir = dataset_path / 'genuine'\n",
    "        genuine_files = list(genuine_dir.glob('*.[jJ][pP][gG]')) + list(genuine_dir.glob('*.[pP][nN][gG]'))\n",
    "        \n",
    "        print(f\"📂 Loading {len(genuine_files)} genuine samples...\")\n",
    "        for i, img_path in enumerate(genuine_files):\n",
    "            img = self.load_and_preprocess_image(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                features.append(self.extract_visual_features(img))\n",
    "                labels.append(1)  # 1 = genuine\n",
    "                filenames.append(img_path.name)\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"   Processed {i + 1}/{len(genuine_files)} genuine samples\")\n",
    "        \n",
    "        print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "        print(f\"   📊 Total samples: {len(images)}\")\n",
    "        print(f\"   📊 Fake samples: {sum(1 for l in labels if l == 0)}\")\n",
    "        print(f\"   📊 Genuine samples: {sum(1 for l in labels if l == 1)}\")\n",
    "        \n",
    "        return {\n",
    "            'images': np.array(images),\n",
    "            'features': np.array(features),\n",
    "            'labels': np.array(labels),\n",
    "            'filenames': filenames\n",
    "        }\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = ImageFeatureExtractor()\n",
    "print(\"🔧 Feature extractor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "# Load and process the dataset\n",
    "print(\"🚀 Starting dataset loading and feature extraction...\")\n",
    "print(\"⏱️  This may take several minutes depending on dataset size...\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = feature_extractor.load_dataset('dataset')\n",
    "\n",
    "# Extract components\n",
    "X_images = dataset['images']\n",
    "X_features = dataset['features']\n",
    "y = dataset['labels']\n",
    "filenames = dataset['filenames']\n",
    "\n",
    "print(f\"\\n📊 Dataset Summary:\")\n",
    "print(f\"   🖼️  Image shape: {X_images.shape}\")\n",
    "print(f\"   🔢 Feature shape: {X_features.shape}\")\n",
    "print(f\"   🏷️  Labels shape: {y.shape}\")\n",
    "print(f\"   📁 Files processed: {len(filenames)}\")\n",
    "\n",
    "# Data distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\n📈 Class Distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    class_name = 'Fake' if label == 0 else 'Genuine'\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"   {class_name}: {count} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "## 🔄 4. Data Splitting & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_data"
   },
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "print(\"🔄 Splitting dataset...\")\n",
    "\n",
    "# Split images and features\n",
    "X_img_train, X_img_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "    X_images, X_features, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_img_train, X_img_val, X_feat_train, X_feat_val, y_train, y_val = train_test_split(\n",
    "    X_img_train, X_feat_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"📊 Data Split Summary:\")\n",
    "print(f\"   🏋️  Training: {len(X_img_train)} samples\")\n",
    "print(f\"   ✅ Validation: {len(X_img_val)} samples\")\n",
    "print(f\"   🧪 Testing: {len(X_img_test)} samples\")\n",
    "\n",
    "# Scale features for Random Forest\n",
    "print(\"\\n⚙️ Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_feat_train_scaled = scaler.fit_transform(X_feat_train)\n",
    "X_feat_val_scaled = scaler.transform(X_feat_val)\n",
    "X_feat_test_scaled = scaler.transform(X_feat_test)\n",
    "\n",
    "print(\"✅ Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "random_forest"
   },
   "source": [
    "## 🌲 5. Random Forest Training & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rf_training"
   },
   "outputs": [],
   "source": [
    "# Random Forest Training with Hyperparameter Tuning\n",
    "print(\"🌲 Training Random Forest Classifier...\")\n",
    "\n",
    "# Define parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Perform GridSearch\n",
    "print(\"🔍 Performing hyperparameter optimization...\")\n",
    "print(\"⏱️  This may take 10-15 minutes...\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_feat_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\n🎯 Best Random Forest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 Best Cross-Validation Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rf_evaluation"
   },
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "print(\"📊 Evaluating Random Forest Model...\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_rf.predict(X_feat_train_scaled)\n",
    "y_val_pred = best_rf.predict(X_feat_val_scaled)\n",
    "y_test_pred = best_rf.predict(X_feat_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_proba = best_rf.predict_proba(X_feat_train_scaled)[:, 1]\n",
    "y_val_proba = best_rf.predict_proba(X_feat_val_scaled)[:, 1]\n",
    "y_test_proba = best_rf.predict_proba(X_feat_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(f\"\\n🎯 Random Forest Performance:\")\n",
    "print(f\"   📈 Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"   📈 Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"   📈 Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   📈 Training AUC: {train_auc:.4f}\")\n",
    "print(f\"   📈 Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"   📈 Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n📋 Detailed Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Fake', 'Genuine']))\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    'Red_Mean', 'Green_Mean', 'Blue_Mean', 'Red_Std', 'Green_Std', 'Blue_Std',\n",
    "    'Brightness', 'Contrast', 'Edge_Density', 'Texture_Variance'\n",
    "]\n",
    "\n",
    "importances = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n🔍 Top 5 Most Important Features:\")\n",
    "for idx, row in feature_importance_df.head().iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_df, x='Importance', y='Feature')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnn_training"
   },
   "source": [
    "## 🧠 6. CNN/Deep Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn_model"
   },
   "outputs": [],
   "source": [
    "# Build CNN Model\n",
    "def create_cnn_model(input_shape=(224, 224, 3)):\n",
    "    \"\"\"Create CNN model for job posting classification\"\"\"\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fourth Convolutional Block\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Dense Layers\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output Layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "print(\"🧠 Creating CNN Model...\")\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "# Compile model\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\n📋 CNN Model Architecture:\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = cnn_model.count_params()\n",
    "print(f\"\\n📊 Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_augmentation"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "print(\"🔄 Setting up data augmentation...\")\n",
    "\n",
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data generator (no augmentation)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Fit generators\n",
    "train_generator = train_datagen.flow(X_img_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_img_val, y_val, batch_size=32)\n",
    "\n",
    "print(\"✅ Data augmentation setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn_training_process"
   },
   "outputs": [],
   "source": [
    "# Training Callbacks\n",
    "print(\"⚙️ Setting up training callbacks...\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_cnn_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n🚀 Starting CNN training...\")\n",
    "print(\"⏱️  This may take 30-60 minutes depending on GPU availability...\")\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_img_train) // 32,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_img_val) // 32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn_evaluation"
   },
   "outputs": [],
   "source": [
    "# Evaluate CNN Model\n",
    "print(\"📊 Evaluating CNN Model...\")\n",
    "\n",
    "# Load best model\n",
    "best_cnn = tf.keras.models.load_model('best_cnn_model.h5')\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_cnn = (best_cnn.predict(X_img_train) > 0.5).astype(int).flatten()\n",
    "y_val_pred_cnn = (best_cnn.predict(X_img_val) > 0.5).astype(int).flatten()\n",
    "y_test_pred_cnn = (best_cnn.predict(X_img_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_proba_cnn = best_cnn.predict(X_img_train).flatten()\n",
    "y_val_proba_cnn = best_cnn.predict(X_img_val).flatten()\n",
    "y_test_proba_cnn = best_cnn.predict(X_img_test).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_cnn = accuracy_score(y_train, y_train_pred_cnn)\n",
    "val_acc_cnn = accuracy_score(y_val, y_val_pred_cnn)\n",
    "test_acc_cnn = accuracy_score(y_test, y_test_pred_cnn)\n",
    "\n",
    "train_auc_cnn = roc_auc_score(y_train, y_train_proba_cnn)\n",
    "val_auc_cnn = roc_auc_score(y_val, y_val_proba_cnn)\n",
    "test_auc_cnn = roc_auc_score(y_test, y_test_proba_cnn)\n",
    "\n",
    "print(f\"\\n🎯 CNN Performance:\")\n",
    "print(f\"   📈 Training Accuracy: {train_acc_cnn:.4f}\")\n",
    "print(f\"   📈 Validation Accuracy: {val_acc_cnn:.4f}\")\n",
    "print(f\"   📈 Test Accuracy: {test_acc_cnn:.4f}\")\n",
    "print(f\"   📈 Training AUC: {train_auc_cnn:.4f}\")\n",
    "print(f\"   📈 Validation AUC: {val_auc_cnn:.4f}\")\n",
    "print(f\"   📈 Test AUC: {test_auc_cnn:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n📋 Detailed Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred_cnn, target_names=['Fake', 'Genuine']))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Learning rate plot (if available)\n",
    "plt.subplot(1, 3, 3)\n",
    "if 'lr' in history.history:\n",
    "    plt.plot(history.history['lr'])\n",
    "    plt.title('Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Learning Rate\\nNot Recorded', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Learning Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_comparison"
   },
   "source": [
    "## 📊 7. Model Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "print(\"📊 Model Comparison Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['Random Forest', 'CNN'],\n",
    "    'Test_Accuracy': [test_acc, test_acc_cnn],\n",
    "    'Test_AUC': [test_auc, test_auc_cnn],\n",
    "    'Val_Accuracy': [val_acc, val_acc_cnn],\n",
    "    'Val_AUC': [val_auc, val_auc_cnn]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['Test_Accuracy'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_accuracy = comparison_df.loc[best_model_idx, 'Test_Accuracy']\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"🎯 Best Test Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Ensemble prediction (average of both models)\n",
    "print(\"\\n🤝 Creating Ensemble Prediction...\")\n",
    "ensemble_proba = (y_test_proba + y_test_proba_cnn) / 2\n",
    "ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_test, ensemble_pred)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_proba)\n",
    "\n",
    "print(f\"🎯 Ensemble Test Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"🎯 Ensemble Test AUC: {ensemble_auc:.4f}\")\n",
    "\n",
    "# Add ensemble to comparison\n",
    "comparison_data['Model'].append('Ensemble')\n",
    "comparison_data['Test_Accuracy'].append(ensemble_acc)\n",
    "comparison_data['Test_AUC'].append(ensemble_auc)\n",
    "comparison_data['Val_Accuracy'].append(np.nan)  # Not calculated for ensemble\n",
    "comparison_data['Val_AUC'].append(np.nan)\n",
    "\n",
    "final_comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n📈 Final Model Comparison:\")\n",
    "print(final_comparison_df.round(4))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "models = ['Random Forest', 'CNN', 'Ensemble']\n",
    "accuracies = [test_acc, test_acc_cnn, ensemble_acc]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "plt.bar(models, accuracies, color=colors)\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# AUC comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "aucs = [test_auc, test_auc_cnn, ensemble_auc]\n",
    "plt.bar(models, aucs, color=colors)\n",
    "plt.title('Test AUC Comparison')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(aucs):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Confusion Matrix for best model\n",
    "plt.subplot(2, 2, 3)\n",
    "if best_model_name == 'Random Forest':\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "else:\n",
    "    cm = confusion_matrix(y_test, y_test_pred_cnn)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake', 'Genuine'], \n",
    "            yticklabels=['Fake', 'Genuine'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Performance metrics radar chart data\n",
    "plt.subplot(2, 2, 4)\n",
    "metrics_rf = [test_acc, test_auc, precision_score(y_test, y_test_pred), recall_score(y_test, y_test_pred)]\n",
    "metrics_cnn = [test_acc_cnn, test_auc_cnn, precision_score(y_test, y_test_pred_cnn), recall_score(y_test, y_test_pred_cnn)]\n",
    "metrics_names = ['Accuracy', 'AUC', 'Precision', 'Recall']\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, metrics_rf, width, label='Random Forest', alpha=0.8)\n",
    "plt.bar(x + width/2, metrics_cnn, width, label='CNN', alpha=0.8)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xticks(x, metrics_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_export"
   },
   "source": [
    "## 💾 8. Model Export & Production Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_models"
   },
   "outputs": [],
   "source": [
    "# Export trained models for production\n",
    "print(\"💾 Exporting models for production...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('production_models', exist_ok=True)\n",
    "\n",
    "# Export Random Forest\n",
    "rf_filename = 'production_models/random_forest_production.pkl'\n",
    "joblib.dump(best_rf, rf_filename)\n",
    "print(f\"✅ Random Forest saved: {rf_filename}\")\n",
    "\n",
    "# Export Feature Scaler\n",
    "scaler_filename = 'production_models/feature_scaler_production.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"✅ Feature Scaler saved: {scaler_filename}\")\n",
    "\n",
    "# Export CNN Model\n",
    "cnn_filename = 'production_models/cnn_production.h5'\n",
    "best_cnn.save(cnn_filename)\n",
    "print(f\"✅ CNN Model saved: {cnn_filename}\")\n",
    "\n",
    "# Create feature names file\n",
    "feature_names_file = 'production_models/feature_names_production.txt'\n",
    "with open(feature_names_file, 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"✅ Feature names saved: {feature_names_file}\")\n",
    "\n",
    "# Create model metadata\n",
    "metadata = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_size': len(y),\n",
    "    'train_size': len(y_train),\n",
    "    'val_size': len(y_val),\n",
    "    'test_size': len(y_test),\n",
    "    'random_forest': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_auc': float(test_auc),\n",
    "        'best_params': grid_search.best_params_\n",
    "    },\n",
    "    'cnn': {\n",
    "        'test_accuracy': float(test_acc_cnn),\n",
    "        'test_auc': float(test_auc_cnn),\n",
    "        'total_params': int(total_params)\n",
    "    },\n",
    "    'ensemble': {\n",
    "        'test_accuracy': float(ensemble_acc),\n",
    "        'test_auc': float(ensemble_auc)\n",
    "    },\n",
    "    'best_model': best_model_name,\n",
    "    'feature_names': feature_names\n",
    "}\n",
    "\n",
    "metadata_file = 'production_models/model_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✅ Model metadata saved: {metadata_file}\")\n",
    "\n",
    "print(f\"\\n🎉 All models exported successfully!\")\n",
    "print(f\"📁 Production files location: production_models/\")\n",
    "print(f\"\\n📋 Files created:\")\n",
    "print(f\"   🌲 {rf_filename}\")\n",
    "print(f\"   ⚙️ {scaler_filename}\")\n",
    "print(f\"   🧠 {cnn_filename}\")\n",
    "print(f\"   📝 {feature_names_file}\")\n",
    "print(f\"   📊 {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_models"
   },
   "source": [
    "## 📥 9. Download Production Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Download production models\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"📦 Creating production models archive...\")\n",
    "\n",
    "# Create zip file with all production models\n",
    "zip_filename = 'cekajayuk_production_models.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # Add all files from production_models directory\n",
    "    for file_path in Path('production_models').glob('*'):\n",
    "        if file_path.is_file():\n",
    "            zipf.write(file_path, file_path.name)\n",
    "            print(f\"   Added: {file_path.name}\")\n",
    "\n",
    "print(f\"\\n✅ Archive created: {zip_filename}\")\n",
    "print(f\"📊 Archive size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Download the zip file\n",
    "print(\"\\n📥 Downloading production models...\")\n",
    "files.download(zip_filename)\n",
    "\n",
    "print(\"\\n🎉 Download completed!\")\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"1. Extract the downloaded zip file\")\n",
    "print(\"2. Copy model files to your CekAjaYuk project's 'models/' directory\")\n",
    "print(\"3. Update your backend to use these production models\")\n",
    "print(\"4. Test the models with your backend API\")\n",
    "\n",
    "print(\"\\n🚀 Your CekAjaYuk models are ready for production!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
